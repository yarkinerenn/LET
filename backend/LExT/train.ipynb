{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.basic_functions import get_prediction\n",
    "from transformers import pipeline\n",
    "from src.utils import save_to_references\n",
    "from metrics.correctness import correctness\n",
    "from metrics.consistency import consistency\n",
    "from metrics.plausibility import plausibility\n",
    "from metrics.QAG import qag\n",
    "from metrics.contextual import contextual_faithfulness\n",
    "from metrics.counterfactual import counterfactual_faithfulness\n",
    "from metrics.faithfulness import faithfulness\n",
    "from metrics.trustworthiness import lext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Models and Helper Keys Initialization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the ner-pipe entitiy from huggingface using which you want your explanations will be tagged. \n",
    "The below ner_pipe is a medical NER model. Other domain-specific NER models can be appropriately used for evaluating explanations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipe = pipeline(\"token-classification\", model=\"Clinical-AI-Apollo/Medical-NER\", aggregation_strategy='simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This evaluation uses groq API for accessing bigger models like llama3-70b. Create an API key from Groq and replace it in the placeholder below. You can also replace the bigger model by going to [src/basic_functions.py](src/basic_functions.py) and replacing it with any othe model from groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api= \"your-groq-api\"\n",
    "model =\"target-model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the question and context, along with the ground label and explanation for computing the LeXT Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I have a fever, should I take a paracetamol. Yes/No?\"\n",
    "context= None \n",
    "ground_label= \"Yes\"\n",
    "ground_explanation= \"Paracetamol is the recommended and is effective for reduction of fever in adults and children. However it is always recommended to consult with a healthcare professional before taking any medication. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Overall LExT Score\n",
    "Run the below function. All the relevant scores will be printed and the results along with the reference data for further analysis will be stored in references.csv. Make sure that the models are pulled locally on ollama and the api keys and pipelines are correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing weighted accuracy\n",
      "\n",
      "Computing Context Relevancy\n",
      "\n",
      "Computing correctness\n",
      "\n",
      "Accuracy: 0.008375763770279323, Context Relevancy: 0.5532126426696777, Correctness: 0.28079420321997856\n",
      "\n",
      "Computing iterative stability\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing paraphrase stability\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing consistency\n",
      "\n",
      "Iterative Stability: 0.9094947076828562, Paraphrase Stability: 0.9135900299589156, Consistency: 0.9115423688208859\n",
      "\n",
      "Computing plausibility\n",
      "\n",
      "Plausibility: 0.5961682860204323\n",
      "\n",
      "Computing QAG score\n",
      "\n",
      "Computing Counterfactual Faithfulness\n",
      "\n",
      "Computing Contextual Faithfulness\n",
      "\n",
      "Computing Faithfulness\n",
      "\n",
      "QAG: 1.0, Counterfactual: 1.0, Contextual: 0, Faithfulness: 0.6666666666666666\n",
      "\n",
      "LExT (Language Explanation Trustworthiness) Score: 0.6294496730042588\n",
      "All model outputs are saved in data/references.csv\n"
     ]
    }
   ],
   "source": [
    "LExT = lext(context, question, ground_explanation, ground_label, model, groq_api, ner_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Individual Metrics\n",
    "\n",
    "To compute any individual or aggregate set of metrics, run the below code to get the sample prediction and create a row reference dictionary that can be used to access and save data by other helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, explanation = get_prediction(context, question, model, groq_api, include_context=\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_reference = {\n",
    "        \"ground_context\": context,\n",
    "        \"ground_question\": question,\n",
    "        \"ground_explanation\": ground_explanation,\n",
    "        \"ground_label\":ground_label,\n",
    "        \"predicted_explanation\": explanation,\n",
    "        \"predicted_label\" : label\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use any of the below functions to compute the individual metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing weighted accuracy\n",
      "\n",
      "Computing Context Relevancy\n",
      "\n",
      "Computing correctness\n",
      "\n",
      "Accuracy: 0.008375763770279323, Context Relevancy: 0.5394374132156372, Correctness: 0.2739065884929583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correctness_score = correctness(ground_explanation, explanation, question, groq_api, ner_pipe, row_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing iterative stability\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing paraphrase stability\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing weighted accuracy\n",
      "\n",
      "Computing consistency\n",
      "\n",
      "Iterative Stability: 0.9509319557815018, Paraphrase Stability: 1.0, Consistency: 0.9754659778907508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consistency_score = consistency(question, context, model, ground_explanation, groq_api, ner_pipe, row_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plausibility_score = plausibility(context, question, ground_explanation, model, groq_api, ner_pipe, row_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qag_score = qag(ground_explanation, groq_api, model, row_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_faithfulness_score = counterfactual_faithfulness(explanation, question, label, model, groq_api, row_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_faithfulness_score = contextual_faithfulness(context, question, label, model, groq_api, row_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_score = faithfulness(explanation, label, question, ground_label, context, groq_api, model, row_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call individual metrics, the evaluation scores along with the model responses and results of the experiments will be saved in row_references dictionary. To save this, you can run the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_references(row_reference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
